{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import tldextract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Related to [T63](https://mattiasostmar.phacility.com/T63) in Phabricator \n",
    "\n",
    "The raw survey-file is first copied to data/interim/<name>_cleaned.txt.\n",
    "\n",
    "That file is thereafter manually altered:\n",
    "\n",
    "- Manually removed first 123 lines, where text was still not stored by script\n",
    "- Manually removed 69 occurances of empty lines\n",
    "\n",
    "This leaves 28 311 lines in survey_2018-01-23_cleaned.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace unwanted semicolons\n",
    "def replace_superflous_semicolons(line, line_no):\n",
    "    \n",
    "    # remove semicolons\n",
    "    semicolons = 0\n",
    "    new_line = \"\"\n",
    "    for char in line:\n",
    "        if char == \";\":\n",
    "            semicolons += 1\n",
    "            \n",
    "        if char == \";\" and semicolons > 11: # the number of expected columns\n",
    "            new_line += \"<semic>\"\n",
    "        else:\n",
    "            new_line += char\n",
    "    \n",
    "    if len(new_line.split(\";\")) < 11:\n",
    "        return new_line.replace(\";\",\"<semic>\")\n",
    "    else:\n",
    "        return new_line\n",
    "\n",
    "def is_orphan_line(line):\n",
    "    return not line.startswith(\"http\")\n",
    "\n",
    "def remove_newlines(line):\n",
    "    return re.sub(r\"[\\n\\r\\n]\",\" \",line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added line 878's text to line 877\n",
      "added line 879's text to line 877\n",
      "added line 992's text to line 991\n",
      "added line 993's text to line 991\n",
      "added line 1077's text to line 1076\n",
      "added line 1434's text to line 1433\n",
      "added line 1620's text to line 1619\n",
      "added line 1621's text to line 1619\n",
      "added line 1634's text to line 1633\n",
      "added line 1635's text to line 1633\n",
      "added line 1636's text to line 1633\n",
      "added line 1637's text to line 1633\n",
      "added line 1638's text to line 1633\n",
      "added line 1639's text to line 1633\n",
      "added line 1640's text to line 1633\n",
      "added line 1641's text to line 1633\n",
      "added line 1642's text to line 1633\n",
      "added line 1643's text to line 1633\n",
      "added line 1644's text to line 1633\n",
      "added line 1645's text to line 1633\n",
      "added line 1646's text to line 1633\n",
      "added line 1647's text to line 1633\n",
      "added line 1648's text to line 1633\n",
      "added line 1649's text to line 1633\n",
      "added line 1998's text to line 1997\n",
      "added line 1999's text to line 1997\n",
      "added line 2001's text to line 2000\n",
      "added line 2002's text to line 2000\n",
      "added line 2010's text to line 2009\n",
      "added line 2083's text to line 2082\n",
      "added line 2084's text to line 2082\n",
      "added line 2085's text to line 2082\n",
      "added line 2086's text to line 2082\n",
      "added line 2087's text to line 2082\n",
      "added line 2088's text to line 2082\n",
      "added line 2105's text to line 2104\n",
      "added line 2280's text to line 2279\n",
      "added line 2281's text to line 2279\n",
      "added line 2282's text to line 2279\n",
      "added line 2283's text to line 2279\n",
      "added line 2284's text to line 2279\n",
      "added line 2285's text to line 2279\n",
      "added line 2286's text to line 2279\n",
      "added line 2287's text to line 2279\n",
      "added line 2401's text to line 2400\n",
      "added line 2445's text to line 2444\n",
      "added line 2450's text to line 2449\n",
      "added line 2572's text to line 2571\n",
      "added line 2573's text to line 2571\n",
      "added line 2631's text to line 2630\n",
      "added line 2635's text to line 2634\n",
      "added line 2718's text to line 2717\n",
      "added line 2719's text to line 2717\n",
      "added line 2720's text to line 2717\n",
      "added line 2721's text to line 2717\n",
      "added line 2722's text to line 2717\n",
      "added line 2723's text to line 2717\n",
      "added line 2848's text to line 2847\n",
      "added line 3093's text to line 3092\n",
      "added line 3291's text to line 3290\n",
      "added line 5029's text to line 5028\n",
      "added line 5030's text to line 5028\n",
      "added line 6302's text to line 6301\n",
      "added line 6303's text to line 6301\n",
      "added line 6489's text to line 6488\n",
      "added line 6490's text to line 6488\n",
      "added line 6491's text to line 6488\n",
      "added line 6609's text to line 6608\n",
      "added line 6610's text to line 6608\n",
      "added line 7257's text to line 7256\n",
      "added line 9258's text to line 9257\n",
      "added line 9259's text to line 9257\n",
      "added line 9260's text to line 9257\n",
      "added line 9261's text to line 9257\n",
      "added line 10935's text to line 10934\n",
      "added line 11028's text to line 11027\n",
      "added line 11774's text to line 11773\n",
      "added line 12093's text to line 12092\n",
      "added line 12094's text to line 12092\n",
      "added line 12095's text to line 12092\n",
      "added line 12521's text to line 12520\n",
      "added line 12662's text to line 12661\n",
      "added line 12738's text to line 12737\n",
      "added line 12739's text to line 12737\n",
      "added line 12740's text to line 12737\n",
      "added line 12741's text to line 12737\n",
      "added line 13123's text to line 13122\n",
      "added line 13124's text to line 13122\n",
      "added line 14274's text to line 14273\n",
      "added line 14682's text to line 14681\n",
      "added line 14758's text to line 14757\n",
      "added line 14759's text to line 14757\n",
      "added line 14761's text to line 14760\n",
      "added line 15048's text to line 15047\n",
      "added line 15049's text to line 15047\n",
      "added line 15050's text to line 15047\n",
      "added line 15051's text to line 15047\n",
      "added line 15640's text to line 15639\n",
      "added line 16094's text to line 16093\n",
      "added line 16456's text to line 16455\n",
      "added line 16457's text to line 16455\n",
      "added line 16458's text to line 16455\n",
      "added line 16459's text to line 16455\n",
      "added line 16791's text to line 16790\n",
      "added line 16792's text to line 16790\n",
      "added line 16814's text to line 16813\n",
      "added line 16815's text to line 16813\n",
      "added line 17145's text to line 17144\n",
      "added line 17146's text to line 17144\n",
      "added line 17147's text to line 17144\n",
      "added line 17148's text to line 17144\n",
      "added line 17266's text to line 17265\n",
      "added line 17267's text to line 17265\n",
      "added line 17440's text to line 17439\n",
      "added line 17971's text to line 17970\n",
      "added line 17988's text to line 17987\n",
      "added line 17989's text to line 17987\n",
      "added line 17990's text to line 17987\n",
      "added line 17991's text to line 17987\n",
      "added line 17992's text to line 17987\n",
      "added line 17993's text to line 17987\n",
      "added line 17994's text to line 17987\n",
      "added line 17995's text to line 17987\n",
      "added line 17996's text to line 17987\n",
      "added line 17997's text to line 17987\n",
      "added line 17998's text to line 17987\n",
      "added line 17999's text to line 17987\n",
      "added line 18000's text to line 17987\n",
      "added line 18001's text to line 17987\n",
      "added line 18041's text to line 18040\n",
      "added line 18042's text to line 18040\n",
      "added line 18043's text to line 18040\n",
      "added line 18044's text to line 18040\n",
      "added line 18045's text to line 18040\n",
      "added line 18046's text to line 18040\n",
      "added line 18047's text to line 18040\n",
      "added line 18048's text to line 18040\n",
      "added line 18049's text to line 18040\n",
      "added line 18050's text to line 18040\n",
      "added line 18051's text to line 18040\n",
      "added line 18052's text to line 18040\n",
      "added line 18398's text to line 18397\n",
      "added line 18399's text to line 18397\n",
      "added line 18400's text to line 18397\n",
      "added line 19500's text to line 19499\n",
      "added line 20327's text to line 20326\n",
      "added line 20328's text to line 20326\n",
      "added line 20329's text to line 20326\n",
      "added line 20330's text to line 20326\n",
      "added line 20331's text to line 20326\n",
      "added line 20332's text to line 20326\n",
      "added line 20333's text to line 20326\n",
      "added line 20334's text to line 20326\n",
      "added line 20335's text to line 20326\n",
      "added line 20336's text to line 20326\n",
      "added line 20337's text to line 20326\n",
      "added line 20338's text to line 20326\n",
      "added line 20339's text to line 20326\n",
      "added line 20340's text to line 20326\n",
      "added line 20341's text to line 20326\n",
      "added line 20438's text to line 20437\n",
      "added line 20439's text to line 20437\n",
      "added line 20440's text to line 20437\n",
      "added line 20441's text to line 20437\n",
      "added line 20442's text to line 20437\n",
      "added line 20443's text to line 20437\n",
      "added line 20444's text to line 20437\n",
      "added line 20446's text to line 20445\n",
      "added line 20447's text to line 20445\n",
      "added line 20448's text to line 20445\n",
      "added line 20449's text to line 20445\n",
      "added line 20450's text to line 20445\n",
      "added line 20451's text to line 20445\n",
      "added line 20452's text to line 20445\n",
      "added line 20620's text to line 20619\n",
      "added line 20621's text to line 20619\n",
      "added line 20622's text to line 20619\n",
      "added line 20623's text to line 20619\n",
      "added line 20624's text to line 20619\n",
      "added line 20625's text to line 20619\n",
      "added line 20626's text to line 20619\n",
      "added line 20627's text to line 20619\n",
      "added line 20628's text to line 20619\n",
      "added line 20783's text to line 20782\n",
      "added line 20784's text to line 20782\n",
      "added line 20785's text to line 20782\n",
      "added line 20786's text to line 20782\n",
      "added line 21422's text to line 21421\n",
      "added line 21423's text to line 21421\n",
      "added line 21424's text to line 21421\n",
      "added line 21426's text to line 21425\n",
      "added line 21879's text to line 21878\n",
      "added line 21880's text to line 21878\n",
      "added line 22167's text to line 22166\n",
      "added line 22168's text to line 22166\n",
      "added line 22169's text to line 22166\n",
      "added line 22170's text to line 22166\n",
      "added line 22260's text to line 22259\n",
      "added line 22458's text to line 22457\n",
      "added line 22459's text to line 22457\n",
      "added line 22690's text to line 22689\n",
      "added line 22812's text to line 22811\n",
      "added line 22813's text to line 22811\n",
      "added line 22814's text to line 22811\n",
      "added line 22815's text to line 22811\n",
      "added line 23074's text to line 23073\n",
      "added line 23075's text to line 23073\n",
      "added line 23076's text to line 23073\n",
      "added line 23077's text to line 23073\n",
      "added line 23078's text to line 23073\n",
      "added line 23079's text to line 23073\n",
      "added line 23437's text to line 23436\n",
      "added line 23438's text to line 23436\n",
      "added line 23439's text to line 23436\n",
      "added line 23440's text to line 23436\n",
      "added line 23441's text to line 23436\n",
      "added line 23442's text to line 23436\n",
      "added line 23444's text to line 23443\n",
      "added line 23445's text to line 23443\n",
      "added line 23446's text to line 23443\n",
      "added line 23447's text to line 23443\n",
      "added line 23448's text to line 23443\n",
      "added line 23449's text to line 23443\n",
      "added line 23456's text to line 23455\n",
      "added line 23457's text to line 23455\n",
      "added line 23458's text to line 23455\n",
      "added line 23460's text to line 23459\n",
      "added line 23461's text to line 23459\n",
      "added line 23466's text to line 23465\n",
      "added line 23467's text to line 23465\n",
      "added line 23468's text to line 23465\n",
      "added line 23469's text to line 23465\n",
      "added line 23470's text to line 23465\n",
      "added line 23471's text to line 23465\n",
      "added line 23472's text to line 23465\n",
      "added line 23473's text to line 23465\n",
      "added line 23474's text to line 23465\n",
      "added line 23475's text to line 23465\n",
      "added line 23476's text to line 23465\n",
      "added line 23477's text to line 23465\n",
      "added line 23478's text to line 23465\n",
      "added line 23479's text to line 23465\n",
      "added line 23480's text to line 23465\n",
      "added line 23481's text to line 23465\n",
      "added line 23482's text to line 23465\n",
      "added line 23483's text to line 23465\n",
      "added line 23484's text to line 23465\n",
      "added line 23485's text to line 23465\n",
      "added line 23486's text to line 23465\n",
      "added line 23487's text to line 23465\n",
      "added line 23488's text to line 23465\n",
      "added line 23489's text to line 23465\n",
      "added line 23490's text to line 23465\n",
      "added line 23491's text to line 23465\n",
      "added line 23990's text to line 23989\n",
      "added line 23991's text to line 23989\n",
      "added line 23992's text to line 23989\n",
      "added line 24148's text to line 24147\n",
      "added line 24149's text to line 24147\n",
      "added line 24348's text to line 24347\n",
      "added line 24349's text to line 24347\n",
      "added line 24350's text to line 24347\n",
      "added line 24351's text to line 24347\n",
      "added line 24352's text to line 24347\n",
      "added line 24353's text to line 24347\n",
      "added line 24354's text to line 24347\n",
      "added line 24355's text to line 24347\n",
      "added line 24356's text to line 24347\n",
      "added line 24358's text to line 24357\n",
      "added line 24363's text to line 24362\n",
      "added line 24365's text to line 24364\n",
      "added line 24366's text to line 24364\n",
      "added line 24367's text to line 24364\n",
      "added line 24369's text to line 24368\n",
      "added line 24370's text to line 24368\n",
      "added line 24371's text to line 24368\n",
      "added line 24372's text to line 24368\n",
      "added line 24374's text to line 24373\n",
      "added line 24377's text to line 24376\n",
      "added line 24378's text to line 24376\n",
      "added line 24380's text to line 24379\n",
      "added line 24808's text to line 24807\n",
      "added line 24809's text to line 24807\n",
      "added line 24810's text to line 24807\n",
      "added line 25105's text to line 25104\n",
      "added line 25106's text to line 25104\n",
      "added line 25107's text to line 25104\n",
      "added line 25108's text to line 25104\n",
      "added line 25109's text to line 25104\n",
      "added line 25111's text to line 25110\n",
      "added line 25112's text to line 25110\n",
      "added line 25113's text to line 25110\n",
      "added line 25230's text to line 25229\n",
      "added line 25231's text to line 25229\n",
      "added line 25232's text to line 25229\n",
      "added line 25233's text to line 25229\n",
      "added line 25234's text to line 25229\n",
      "added line 25235's text to line 25229\n",
      "added line 25236's text to line 25229\n",
      "added line 25237's text to line 25229\n",
      "added line 25238's text to line 25229\n",
      "added line 25239's text to line 25229\n",
      "added line 25240's text to line 25229\n",
      "added line 25241's text to line 25229\n",
      "added line 25242's text to line 25229\n",
      "added line 25243's text to line 25229\n",
      "added line 25244's text to line 25229\n",
      "added line 25245's text to line 25229\n",
      "added line 25246's text to line 25229\n",
      "added line 25391's text to line 25390\n",
      "added line 25457's text to line 25456\n",
      "added line 25458's text to line 25456\n",
      "added line 25561's text to line 25560\n",
      "added line 25577's text to line 25576\n",
      "added line 25578's text to line 25576\n",
      "added line 25579's text to line 25576\n",
      "added line 25580's text to line 25576\n",
      "added line 25725's text to line 25724\n",
      "added line 25726's text to line 25724\n",
      "added line 25727's text to line 25724\n",
      "added line 25728's text to line 25724\n",
      "added line 25729's text to line 25724\n",
      "added line 25758's text to line 25757\n",
      "added line 25759's text to line 25757\n",
      "added line 25760's text to line 25757\n",
      "added line 25761's text to line 25757\n",
      "added line 25762's text to line 25757\n",
      "added line 25763's text to line 25757\n",
      "added line 25764's text to line 25757\n",
      "added line 25765's text to line 25757\n",
      "added line 25766's text to line 25757\n",
      "added line 25767's text to line 25757\n",
      "added line 25768's text to line 25757\n",
      "added line 25769's text to line 25757\n",
      "added line 25770's text to line 25757\n",
      "added line 25772's text to line 25771\n",
      "added line 25773's text to line 25771\n",
      "added line 25774's text to line 25771\n",
      "added line 25775's text to line 25771\n",
      "added line 25776's text to line 25771\n",
      "added line 25777's text to line 25771\n",
      "added line 25778's text to line 25771\n",
      "added line 25779's text to line 25771\n",
      "added line 25780's text to line 25771\n",
      "added line 25781's text to line 25771\n",
      "added line 25782's text to line 25771\n",
      "added line 25783's text to line 25771\n",
      "added line 25784's text to line 25771\n",
      "added line 26109's text to line 26108\n",
      "added line 26110's text to line 26108\n",
      "added line 26111's text to line 26108\n",
      "len(data): 27959\n"
     ]
    }
   ],
   "source": [
    "f_in = open(\"../data/interim/survey_2018-01-23_to_clean.txt\")\n",
    "f_out = open(\"../data/processed/survey_2018-01-23_cleaned.txt\",\"w\")\n",
    "line_no = 0\n",
    "\n",
    "data = {}\n",
    "\n",
    "# ['url', 'typealyzer', 'actual', 'e', 's', 't', 'sntf_s', 'sntf_n', 'sntf_t', 'sntf_f', 'date', 'text']\n",
    "cols = [col.rstrip() for col in f_in.readline().split(\";\")]\n",
    "last_good_line = 1 # Not 0, since it's the columns line\n",
    "\n",
    "for line in f_in:\n",
    "    line_no += 1\n",
    "   \n",
    "    new_line = replace_superflous_semicolons(line, line_no)\n",
    "    new_line = remove_newlines(new_line)\n",
    "    \n",
    "    if line_no > 0 and not is_orphan_line(new_line):\n",
    "        last_good_line = line_no\n",
    "        line_data = new_line.split(\";\")\n",
    "        if not len(line_data) == 12:\n",
    "            print(\"Oops, the line data has len() {}\".format(len(line_data)))\n",
    "            print(\"line_no: {}\".format(line_no))\n",
    "            print(\"new_line: {}\".format(new_line))\n",
    "            print(\"line_data: {}\".format(\"\\nline:\".join(line_data)))\n",
    "            break\n",
    "        data[line_no] = defaultdict(dict)\n",
    "        for col in cols:\n",
    "            data[line_no][col] = None\n",
    "            \n",
    "        for (col,coldata) in zip(cols,new_line.split(\";\")):\n",
    "            data[line_no][col] = coldata\n",
    "        \n",
    "    if is_orphan_line(new_line):\n",
    "        #print(\"Suspected orphan line:\\n{}\".format(new_line))\n",
    "        #print(\"Last good lines fields:\\n{}\".format(data[last_good_line]))\n",
    "        new_line = new_line.strip(\"\\n\")\n",
    "        padded_new_line = \" \" + new_line\n",
    "        data[last_good_line][\"text\"] += padded_new_line.strip(\"\\n\")\n",
    "        print(\"added line {}'s text to line {}\".format(line_no, last_good_line))\n",
    "\n",
    "columns_line = open(\"../data/interim/survey_2018-01-23_cleaned.txt\").readline()\n",
    "f_out.write(columns_line)\n",
    "print(\"len(data): {}\".format(len(data)))\n",
    "#print(\"data[1]:\\n{}\".format(data[1]))\n",
    "for row_nr in data:\n",
    "    row_string = \"\"\n",
    "    for key in cols:\n",
    "        separated_data = data[row_nr][key] + \";\"\n",
    "        row_string += separated_data\n",
    "    row_string = row_string.strip(\";\")\n",
    "    if row_string.endswith(\"\\n\"):\n",
    "        f_out.write(row_string)\n",
    "    else:\n",
    "        f_out.write(row_string + \"\\n\")\n",
    "f_out.close()\n",
    "f_in.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load final cleaned survey-file to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27959 entries, 0 to 27958\n",
      "Data columns (total 12 columns):\n",
      "url           27959 non-null object\n",
      "typealyzer    27959 non-null object\n",
      "actual        27959 non-null object\n",
      "e             27959 non-null float64\n",
      "s             27959 non-null float64\n",
      "t             27959 non-null float64\n",
      "sntf_s        27959 non-null float64\n",
      "sntf_n        27959 non-null float64\n",
      "sntf_t        27959 non-null float64\n",
      "sntf_f        27959 non-null float64\n",
      "date          27959 non-null object\n",
      "text          27959 non-null object\n",
      "dtypes: float64(7), object(5)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "f = open(\"../data/processed/survey_2018-01-23_final.txt\")\n",
    "df = pd.read_csv(f, sep=\";\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>typealyzer</th>\n",
       "      <th>actual</th>\n",
       "      <th>e</th>\n",
       "      <th>s</th>\n",
       "      <th>t</th>\n",
       "      <th>sntf_s</th>\n",
       "      <th>sntf_n</th>\n",
       "      <th>sntf_t</th>\n",
       "      <th>sntf_f</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://jonkagstrom.com</td>\n",
       "      <td>ISTP</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>0.420758</td>\n",
       "      <td>0.651605</td>\n",
       "      <td>0.652214</td>\n",
       "      <td>0.512359</td>\n",
       "      <td>0.274234</td>\n",
       "      <td>0.134025</td>\n",
       "      <td>0.079382</td>\n",
       "      <td>20120828 09:08:55</td>\n",
       "      <td>Jon Kågström playchilla.com uclassify.com abou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://adropofcolour.tumblr.com</td>\n",
       "      <td>ISFP</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>0.291281</td>\n",
       "      <td>0.787844</td>\n",
       "      <td>0.460961</td>\n",
       "      <td>0.663515</td>\n",
       "      <td>0.178565</td>\n",
       "      <td>0.069282</td>\n",
       "      <td>0.088638</td>\n",
       "      <td>20120828 08:08:11</td>\n",
       "      <td>❀*a drop of colour*❀ 1/39 next→ home ask past ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://godheadcomplex.tumblr.com</td>\n",
       "      <td>ESFP</td>\n",
       "      <td>INFP</td>\n",
       "      <td>0.883579</td>\n",
       "      <td>0.951693</td>\n",
       "      <td>0.238407</td>\n",
       "      <td>0.855921</td>\n",
       "      <td>0.046931</td>\n",
       "      <td>0.021850</td>\n",
       "      <td>0.075297</td>\n",
       "      <td>20120828 09:08:34</td>\n",
       "      <td>Neko cool kids can't die home family daveblog ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                url typealyzer actual         e         s  \\\n",
       "0            http://jonkagstrom.com       ISTP   INFJ  0.420758  0.651605   \n",
       "1   http://adropofcolour.tumblr.com       ISFP   INFJ  0.291281  0.787844   \n",
       "2  http://godheadcomplex.tumblr.com       ESFP   INFP  0.883579  0.951693   \n",
       "\n",
       "          t    sntf_s    sntf_n    sntf_t    sntf_f               date  \\\n",
       "0  0.652214  0.512359  0.274234  0.134025  0.079382  20120828 09:08:55   \n",
       "1  0.460961  0.663515  0.178565  0.069282  0.088638  20120828 08:08:11   \n",
       "2  0.238407  0.855921  0.046931  0.021850  0.075297  20120828 09:08:34   \n",
       "\n",
       "                                                text  \n",
       "0  Jon Kågström playchilla.com uclassify.com abou...  \n",
       "1  ❀*a drop of colour*❀ 1/39 next→ home ask past ...  \n",
       "2  Neko cool kids can't die home family daveblog ...  "
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract domains using tldextract\n",
    "See [tldextract](https://pypi.python.org/pypi/tldextract)\n",
    "\n",
    "Note:\n",
    "co.vu is a free domain name service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tumblr                24487\n",
       "blogspot                590\n",
       "wordpress               545\n",
       "co                      285\n",
       "twitter                 248\n",
       "facebook                170\n",
       "google                   63\n",
       "livejournal              53\n",
       "Tumblr                   29\n",
       "weebly                   28\n",
       "reddit                   27\n",
       "okcupid                  21\n",
       "TUMBLR                   20\n",
       "fanfiction               20\n",
       "intjforum                16\n",
       "tumbr                    16\n",
       "Twitter                  15\n",
       "deviantart               15\n",
       "personalitycafe          15\n",
       "youtube                  14\n",
       "typealyzer               13\n",
       "dreamwidth               13\n",
       "personalityjunkie        13\n",
       "medium                   13\n",
       "pointlesssites           12\n",
       "pinterest                12\n",
       "instagram                12\n",
       "ovh                      12\n",
       "fighunter                11\n",
       "pastebin                 10\n",
       "                      ...  \n",
       "yunglean                  1\n",
       "tvmoviechristmas          1\n",
       "ducklings                 1\n",
       "genealogy                 1\n",
       "luteces                   1\n",
       "webofnarcissism           1\n",
       "tagonist                  1\n",
       "glennaoverman             1\n",
       "similarminds              1\n",
       "nicklong                  1\n",
       "outfresh                  1\n",
       "Typealyzer                1\n",
       "muzy                      1\n",
       "susiemakessupper          1\n",
       "chantelc                  1\n",
       "nickolas360               1\n",
       "tomblr                    1\n",
       "truehearted               1\n",
       "madeinthemoment           1\n",
       "perchedonawhim            1\n",
       "lorastyrell               1\n",
       "adprofs                   1\n",
       "voiceoftheheart           1\n",
       "persvarld                 1\n",
       "theslightestspark         1\n",
       "KullbergUtveckling        1\n",
       "annahiatt                 1\n",
       "withoutsushi              1\n",
       "dayre                     1\n",
       "donathandesign            1\n",
       "Name: domain, Length: 910, dtype: int64"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domains = []\n",
    "for index, row in df.iterrows():\n",
    "    ext = tldextract.extract(row.url)\n",
    "    domains.append(ext.domain)\n",
    "df[\"domain\"] = pd.Series(domains)\n",
    "df.domain.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"../pickles/dataframe_survey_2018-01-23_final.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27959 entries, 0 to 27958\n",
      "Data columns (total 14 columns):\n",
      "url           27959 non-null object\n",
      "typealyzer    27959 non-null object\n",
      "actual        27959 non-null object\n",
      "e             27959 non-null float64\n",
      "s             27959 non-null float64\n",
      "t             27959 non-null float64\n",
      "sntf_s        27959 non-null float64\n",
      "sntf_n        27959 non-null float64\n",
      "sntf_t        27959 non-null float64\n",
      "sntf_f        27959 non-null float64\n",
      "date          27959 non-null object\n",
      "text          27959 non-null object\n",
      "domains       27959 non-null object\n",
      "domain        27959 non-null object\n",
      "dtypes: float64(7), object(7)\n",
      "memory usage: 3.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
